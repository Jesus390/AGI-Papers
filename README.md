# NLP-Papers
- [Basic Knowledge](https://github.com/newhiwoong/NLP-Papers#basic-knowledge)
- [Start NLP](https://github.com/newhiwoong/NLP-Papers#Start-NLP)
- [Transformer](https://github.com/newhiwoong/NLP-Papers#Transformer)
- [Transformer-XL](https://github.com/newhiwoong/NLP-Papers#Transformer-XL)

## Basic knowledge

- [mathematics for machine learning](https://mml-book.github.io/book/mml-book.pdf)
- [Pattern Recognition and Machine Learning](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf)

## Start NLP

- [Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/pdf/1801.06146.pdf)

## Transformer

- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf), Transformer
- [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf), GPT-1
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)
- [Language Models are Unsupervised Multitask Learners
](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), GPT-2

## Transformer-XL

- [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)
- [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)
