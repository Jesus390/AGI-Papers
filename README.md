# NLP-Papers

## Basic knowledge

- [mathematics for machine learning](https://mml-book.github.io/book/mml-book.pdf)
- [Pattern Recognition and Machine Learning](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf)

## Start NLP

- [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf), 2013/01	
- [Dependency-Based Word Embeddings](https://www.aclweb.org/anthology/P14-2050.pdf), 2014/12	
- [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf), GloVe, 2014/04?
- [Siamese CBOW: Optimizing Word Embeddings for Sentence Representations](https://arxiv.org/pdf/1606.04640.pdf), Siamese CBOW, 2016/06
- [Enriching Word Vectors with Subword Information](https://arxiv.org/pdf/1607.04606.pdf), fastText, 2016/07

- [Sequence to Sequence Learningwith Neural Networks](https://arxiv.org/pdf/1409.3215.pdf), seq2seq, 2014/09

- [Learned in Translation: Contextualized Word Vectors](http://papers.nips.cc/paper/7209-learned-in-translation-contextualized-word-vectors.pdf), CoVe, 2017/08	

- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf), Transformer ,2017/??

- [Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/pdf/1801.06146.pdf), ULMFIT, 2018/01
- [Deep contextualized word representations](https://arxiv.org/pdf/1802.05365.pdf), ELMo, 2018/02
- [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf), GPT-1, 2018/??	
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf), 2018/10	
- [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), GPT-2, 2019/02
- [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860), 2019/01	

- [Cross-lingual Language Model Pretraining](https://arxiv.org/pdf/1901.07291.pdf), XLM, 2019/01	
- [Multi-Task Deep Neural Networks for Natural Language Understanding](https://arxiv.org/pdf/1901.11504.pdf), MT-DNN, 2019/01	

- [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237) 2019/06

- [BioBERT: a pre-trained biomedical language representation model for biomedical text mining](https://arxiv.org/ftp/arxiv/papers/1901/1901.08746.pdf), BioBERT, 2019/01, 2019/01
- [SciBERT: A Pretrained Language Model for Scientific Text](https://arxiv.org/abs/1903.10676.pdf), 2019/03
- [Publicly Available Clinical BERT Embeddings](https://arxiv.org/abs/1904.03323.pdf), 2019/04
- [ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission](https://arxiv.org/abs/1904.05342.pdf), 2019/04
- [HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization](https://arxiv.org/pdf/1905.06566.pdf), 2019/06
- [Pre-Training with Whole Word Masking for Chinese BERT](https://arxiv.org/pdf/1906.08101.pdf), 2019/08

- [R-Transformer: Recurrent Neural Network Enhanced Transformer](https://arxiv.org/abs/1907.05572), 2019/07



